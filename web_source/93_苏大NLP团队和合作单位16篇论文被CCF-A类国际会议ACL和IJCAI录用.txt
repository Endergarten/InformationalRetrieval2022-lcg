<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="renderer" content="webkit" />
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width,user-scalable=0,initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0"/>
<title>苏大NLP团队和合作单位16篇论文被CCF-A类国际会议ACL和IJCAI录用</title>
<meta name="keywords" content="苏州大学计算机科学与技术学院" >
<meta name="description" content="苏州大学计算机科学与技术学院" >
<meta name="description" content="苏州大学自然语言处理团队和合作单位近期在CCF-A类国际会议ACL发表13篇论文，包括7篇长文、3篇短文、和3篇Findings长文；在IJCAI发表3篇长文。文章涵盖的领域包括词法句法分析、语义分析、命名实体识别、篇章分析、阅读理解、文本生成、机器翻译、摘要、预训练和对话评测。自然语言处理（NLP）领域的顶级国际会议ACL-IJCNLP 2021（The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing）将于2021年8月2-4日在线举行（原定在泰国曼谷举行），一年一度的全球学术大会ACL是自然语言处理领域最受关注的国际学术会议之一。本届ACL 2021引入了新在线附属出版物“Findings of ACL”。ACL-IJCNLP 2021共收到有效投稿3350篇，其中21.3%的论文被ACL主会录取，" />

<link type="text/css" href="/_css/_system/system.css" rel="stylesheet"/>
<link type="text/css" href="/_upload/site/1/style/1/1.css" rel="stylesheet"/>
<link type="text/css" href="/_upload/site/01/21/289/style/282/282.css" rel="stylesheet"/>
<link type="text/css" href="/_js/_portletPlugs/sudyNavi/css/sudyNav.css" rel="stylesheet" />
<link type="text/css" href="/_js/_portletPlugs/datepicker/css/datepicker.css" rel="stylesheet" />
<link type="text/css" href="/_js/_portletPlugs/simpleNews/css/simplenews.css" rel="stylesheet" />

<script language="javascript" src="/_js/jquery.min.js" sudy-wp-context="" sudy-wp-siteId="289"></script>
<script language="javascript" src="/_js/jquery.sudy.wp.visitcount.js"></script>
<script type="text/javascript" src="/_js/_portletPlugs/sudyNavi/jquery.sudyNav.js"></script>
<script type="text/javascript" src="/_js/_portletPlugs/datepicker/js/jquery.datepicker.js"></script>
<script type="text/javascript" src="/_js/_portletPlugs/datepicker/js/datepicker_lang_HK.js"></script>
<script type="text/javascript" src="/_upload/tpl/01/db/475/template475/extends/extends.js"></script>
<link href="/_upload/tpl/01/db/475/template475/style.css" rel="stylesheet">
<link href="/_upload/tpl/01/db/475/template475/mobile.css" rel="stylesheet">
<link href="/_upload/tpl/01/db/475/template475/media.css" rel="stylesheet">
<!--[if lt IE 9]>
	<script src="/_upload/tpl/01/db/475/template475/extends/libs/html5.js"></script>
<![endif]-->
</head>
<body class="list">
<div class="wrapper head" id="header">
	<div class="inner">
		<div class="mod clearfix">
			<div class="head-left">
				<!--logo开始-->
				<div class="sitelogo" frag="窗口01" portletmode="simpleSiteAttri">
					<a href="/main.htm" title="返回计算机科学与技术学院首页"><img src="/_upload/tpl/01/db/475/template475/images/logo2.png"/><span class="name"></span></a> 
				</div>
				<!--//logo结束-->		
			</div>
			<div class="head-right">
				<!--<div class="searchbtn"></div>-->
				<div class="searchbox" frag="窗口03" portletmode="search">
											<!--搜索组件-->
						<div class="wp-search clearfix">
							<form action="/_web/_search/api/search/new.rst?locale=zh_CN&request_locale=zh_CN&_p=YXM9Mjg5JnQ9NDc1JmQ9MTY4NSZwPTMmZj0xMDkyMiZhPTAmbT1TTiZ8Ym5uQ29sdW1uVmlydHVhbE5hbWU9MTA5MjIm" method="post" target="_blank">
								<div class="search-input">
									<input name="keyword" class="search-title" type="text" placeholder="请输入关键词..."/>
								</div>
								<div class="search-btn">
									<input name="submit" class="search-submit" type="submit" value=""/>
								</div>
							</form>
						</div>
						<!--//复制以上代码到自定义搜索-->
					
				</div>			
			</div>

		</div>
	</div>
</div>
<!--//头部结束-->
<!--导航开始-->
<nav class="wrapper nav wp-navi" id="nav">
	<div class="inner clearfix">
		<div class="wp-panel main-nav-panel panel-5" frag="面板1">
			<div class="wp-window main-nav-window window-5" frag="窗口1">
				
					<div class="navi-slide-head clearfix">
						<h3 class="navi-slide-title"> 导航 </h3>
						<a class="navi-slide-arrow"></a> </div>
					
					<ul class="wp-menu " data-nav-aside='{"title":"导航","index":0}'>
						
						<li class="menu-item i1"> <a class="menu-link" href="http://scst.suda.edu.cn/main.htm" target="_self">首 页</a>
							
						</li>
						
						<li class="menu-item i2"> <a class="menu-link" href="/11195/list.htm" target="_self">学院概况</a>
							
							<em class="menu-switch-arrow"></em>
							<ul class="sub-menu clearfix">
								
								<li class="sub-item i2-1"><a class="sub-link" href="/11238/list.htm" target="_self">学院简介</a>
									
								</li>
								
								<li class="sub-item i2-2"><a class="sub-link" href="/11241/list.htm" target="_self">历史沿革</a>
									
								</li>
								
								<li class="sub-item i2-3"><a class="sub-link" href="/11240/list.htm" target="_self">现任领导</a>
									
								</li>
								
								<li class="sub-item i2-4"><a class="sub-link" href="/11239/list.htm" target="_self">机构设置</a>
									
								</li>
								
								<li class="sub-item i2-5"><a class="sub-link" href="/11242/list.htm" target="_self">办学资源</a>
									
								</li>
								
								<li class="sub-item i2-6"><a class="sub-link" href="/11243/list.htm" target="_self">联系方式</a>
									
								</li>
								
							</ul>
							
						</li>
						
						<li class="menu-item i3"> <a class="menu-link" href="/11196/list.htm" target="_self"> 学科建设</a>
							
							<em class="menu-switch-arrow"></em>
							<ul class="sub-menu clearfix">
								
								<li class="sub-item i3-1"><a class="sub-link" href="/11244/list.htm" target="_self">江苏省优势学科</a>
									
								</li>
								
								<li class="sub-item i3-2"><a class="sub-link" href="/11245/list.htm" target="_self"> 博士点</a>
									
								</li>
								
								<li class="sub-item i3-3"><a class="sub-link" href="/11246/list.htm" target="_self"> 硕士点</a>
									
								</li>
								
								<li class="sub-item i3-4"><a class="sub-link" href="/11247/list.htm" target="_self"> 实验室安全</a>
									
								</li>
								
							</ul>
							
						</li>
						
						<li class="menu-item i4"> <a class="menu-link" href="/11197/list.htm" target="_self">师资队伍　</a>
							
							<em class="menu-switch-arrow"></em>
							<ul class="sub-menu clearfix">
								
								<li class="sub-item i4-1"><a class="sub-link" href="/11248/list.htm" target="_self">师资概况</a>
									
								</li>
								
								<li class="sub-item i4-2"><a class="sub-link" href="/11249/list.htm" target="_self"> 教职工名单</a>
									
								</li>
								
								<li class="sub-item i4-3"><a class="sub-link" href="/11250/list.htm" target="_self"> 导师简介</a>
									
								</li>
								
								<li class="sub-item i4-4"><a class="sub-link" href="/11251/list.htm" target="_self"> 兼职师资</a>
									
								</li>
								
							</ul>
							
						</li>
						
						<li class="menu-item i5"> <a class="menu-link" href="/11198/list.htm" target="_self"> 科学研究</a>
							
							<em class="menu-switch-arrow"></em>
							<ul class="sub-menu clearfix">
								
								<li class="sub-item i5-1"><a class="sub-link" href="/yjzx/list.htm" target="_self">研究中心</a>
									
							<em class="menu-switch-arrow"></em>
							<ul class="sub-menu clearfix">
								
								<li class="sub-item i5-1-1"><a class="sub-link" href="/28250/list.htm" target="_self">自然语言处理研究中心</a>
									
								</li>
								
								<li class="sub-item i5-1-2"><a class="sub-link" href="/28251/list.htm" target="_self">机器学习与图像处理研究中心</a>
									
								</li>
								
								<li class="sub-item i5-1-3"><a class="sub-link" href="/28252/list.htm" target="_self">数据科学与工业软件研究中心</a>
									
								</li>
								
								<li class="sub-item i5-1-4"><a class="sub-link" href="/28253/list.htm" target="_self">网络科学与工程研究中心</a>
									
								</li>
								
								<li class="sub-item i5-1-5"><a class="sub-link" href="/28254/list.htm" target="_self">人工智能研究院</a>
									
								</li>
								
								<li class="sub-item i5-1-6"><a class="sub-link" href="/28255/list.htm" target="_self">嵌入式技术及系统能力培养研究中心</a>
									
								</li>
								
							</ul>
							
								</li>
								
								<li class="sub-item i5-2"><a class="sub-link" href="/11253/list.htm" target="_self"> 科研机构</a>
									
								</li>
								
								<li class="sub-item i5-3"><a class="sub-link" href="/11254/list.htm" target="_self"> 科研项目</a>
									
								</li>
								
								<li class="sub-item i5-4"><a class="sub-link" href="/11255/list.htm" target="_self"> 科研论文</a>
									
								</li>
								
								<li class="sub-item i5-5"><a class="sub-link" href="/11256/list.htm" target="_self">知识产权</a>
									
								</li>
								
								<li class="sub-item i5-6"><a class="sub-link" href="/11257/list.htm" target="_self"> 科研政策</a>
									
								</li>
								
							</ul>
							
						</li>
						
						<li class="menu-item i6"> <a class="menu-link" href="/11199/list.htm" target="_self">人才培养</a>
							
							<em class="menu-switch-arrow"></em>
							<ul class="sub-menu clearfix">
								
								<li class="sub-item i6-1"><a class="sub-link" href="/11229/list.htm" target="_self">博士生</a>
									
								</li>
								
								<li class="sub-item i6-2"><a class="sub-link" href="/11230/list.htm" target="_self"> 硕士生</a>
									
								</li>
								
								<li class="sub-item i6-3"><a class="sub-link" href="/11231/list.htm" target="_self"> 本科生</a>
									
							<em class="menu-switch-arrow"></em>
							<ul class="sub-menu clearfix">
								
								<li class="sub-item i6-3-1"><a class="sub-link" href="/25067/list.htm" target="_self">人才培养方案</a>
									
								</li>
								
								<li class="sub-item i6-3-2"><a class="sub-link" href="/25068/list.htm" target="_self">课程教学大纲</a>
									
								</li>
								
							</ul>
							
								</li>
								
								<li class="sub-item i6-4"><a class="sub-link" href="/11234/list.htm" target="_self"> 图灵班</a>
									
								</li>
								
								<li class="sub-item i6-5"><a class="sub-link" href="/gjjl/list.htm" target="_self">国际交流</a>
									
								</li>
								
							</ul>
							
						</li>
						
						<li class="menu-item i7"> <a class="menu-link" href="/11200/list.htm" target="_self">学生工作</a>
							
							<em class="menu-switch-arrow"></em>
							<ul class="sub-menu clearfix">
								
								<li class="sub-item i7-1"><a class="sub-link" href="/28212/list.htm" target="_self">工作队伍</a>
									
								</li>
								
								<li class="sub-item i7-2"><a class="sub-link" href="/11275/list.htm" target="_self"> 学工通知</a>
									
								</li>
								
								<li class="sub-item i7-3"><a class="sub-link" href="/xgxw/list.htm" target="_self">学工新闻</a>
									
								</li>
								
								<li class="sub-item i7-4"><a class="sub-link" href="/28213/list.htm" target="_self">规章制度</a>
									
								</li>
								
								<li class="sub-item i7-5"><a class="sub-link" href="/28216/list.htm" target="_self">志愿服务</a>
									
								</li>
								
								<li class="sub-item i7-6"><a class="sub-link" href="/28218/list.htm" target="_self">学生组织</a>
									
								</li>
								
								<li class="sub-item i7-7"><a class="sub-link" href="/28219/list.htm" target="_self">勤工助学</a>
									
								</li>
								
								<li class="sub-item i7-8"><a class="sub-link" href="/28220/list.htm" target="_self">心理健康</a>
									
								</li>
								
								<li class="sub-item i7-9"><a class="sub-link" href="/28221/list.htm" target="_self">职业规划</a>
									
								</li>
								
								<li class="sub-item i7-10"><a class="sub-link" href="/czpb/list.htm" target="_self">成长陪伴</a>
									
								</li>
								
							</ul>
							
						</li>
						
						<li class="menu-item i8"> <a class="menu-link" href="/11201/list.htm" target="_self">党群工作</a>
							
							<em class="menu-switch-arrow"></em>
							<ul class="sub-menu clearfix">
								
								<li class="sub-item i8-1"><a class="sub-link" href="/jczz/list.htm" target="_self">基层组织</a>
									
								</li>
								
								<li class="sub-item i8-2"><a class="sub-link" href="/11260/list.htm" target="_self"> 规章制度</a>
									
								</li>
								
								<li class="sub-item i8-3"><a class="sub-link" href="/11262/list.htm" target="_self"> 专题活动</a>
									
								</li>
								
								<li class="sub-item i8-4"><a class="sub-link" href="/fdxgz/list.htm" target="_self">分党校工作</a>
									
								</li>
								
								<li class="sub-item i8-5"><a class="sub-link" href="/11264/list.htm" target="_self"> 发展公示</a>
									
								</li>
								
								<li class="sub-item i8-6"><a class="sub-link" href="/11265/list.htm" target="_self"> 关工委工作</a>
									
								</li>
								
								<li class="sub-item i8-7"><a class="sub-link" href="/ghgz/list.htm" target="_self">工会工作</a>
									
								</li>
								
							</ul>
							
						</li>
						
					</ul>
					
				
			</div>
			
		</div>
	</div>
</nav>
<!--aside导航-->
<div class="wp-navi-aside" id="wp-navi-aside">
  <div class="aside-inner">
    <div class="navi-aside-wrap"></div>
  </div>
  <div class="navi-aside-mask"></div>
</div>
<!--//导航结束-->
<div class="wp-wrapper" id="container-1">
    <div class="wp-inner l-banner">
        <img style="margin:0 auto;" src="/_upload/tpl/01/db/475/template475/images/list-bg.jpg"/> 
    </div>
</div>
<!--主体开始-->
<div class="wrapper" id="d-container">
	<div class="inner clearfix">
		<div class="infobox" frag="面板3">
			<div class="article" frag="窗口3" portletmode="simpleArticleAttri">
								  <h1 class="arti_title">苏大NLP团队和合作单位16篇论文被CCF-A类国际会议ACL和IJCAI录用</h1>
				  <p class="arti_metas"><span class="arti_publisher">发布者：综合办公室</span><span class="arti_update">发布时间：2021-05-13</span><span class="arti_views">浏览次数：<span class="WP_VisitCount" url="/_visitcountdisplay?siteId=289&type=3&articleId=444006">1352</span></span></p>
				  <div class="entry">
					<div class="read"><div class='wp_articlecontent'><p><style type="text/css"> 	 	</style></p><p style="widows:2;orphans:2;text-align:left;margin-top:0.02in;margin-bottom:0.02in;"><br /></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">苏州大学自然语言处理团队和合作单位近期在</span><span style="font-size:16px;font-family:微软雅黑, serif;">CCF-A</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">类国际会议</span><span style="font-size:16px;font-family:微软雅黑, serif;">ACL</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">发表</span><span style="font-size:16px;font-family:微软雅黑, serif;">13</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">篇论文，包括</span><span style="font-size:16px;font-family:微软雅黑, serif;">7</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">篇长文、</span><span style="font-size:16px;font-family:微软雅黑, serif;">3</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">篇短文、和</span><span style="font-size:16px;font-family:微软雅黑, serif;">3</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">篇</span><span style="font-size:16px;font-family:微软雅黑, serif;">Findings</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">长文；在</span><span style="font-size:16px;font-family:微软雅黑, serif;">IJCAI</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">发表</span><span style="font-size:16px;font-family:微软雅黑, serif;">3</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">篇长文。文章涵盖的领域包括词法句法分析、语义分析、命名实体识别、篇章分析、阅读理解、文本生成、机器翻译、摘要、预训练和对话评测。</span></p><p style="widows:2;orphans:2;text-align:center;margin-top:0.02in;margin-bottom:0.02in;"><img src="/_upload/article/images/16/c2/7c645da44504bd668c6993a6dc79/701a40aa-a2b4-4f3f-89f4-d94dc367cb74.png" align="BOTTOM" width="554" height="150" border="0" /></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">自然语言处理&nbsp;（</span><span style="font-size:16px;font-family:微软雅黑, serif;">NLP</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">）领域的顶级国际会议</span><span style="font-size:16px;font-family:微软雅黑, serif;">ACL-IJCNLP 2021</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">（</span><span style="font-size:16px;font-family:微软雅黑, serif;">The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">）将于</span><span style="font-size:16px;font-family:微软雅黑, serif;">2021</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">年</span><span style="font-size:16px;font-family:微软雅黑, serif;">8</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">月</span><span style="font-size:16px;font-family:微软雅黑, serif;">2-4</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">日在线举行（原定在泰国曼谷举行），一年一度的全球学术大会</span><span style="font-size:16px;font-family:微软雅黑, serif;">ACL</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">是自然语言处理领域最受关注的国际学术会议之一。本届</span><span style="font-size:16px;font-family:微软雅黑, serif;">ACL 2021</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">引入了新在线附属出版物“</span><span style="font-size:16px;font-family:微软雅黑, serif;">Findings of ACL”</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">。</span><span style="font-size:16px;font-family:微软雅黑, serif;">ACL-IJCNLP 2021</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">共收到有效投稿</span><span style="font-size:16px;font-family:微软雅黑, serif;">3350</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">篇，其中</span><span style="font-size:16px;font-family:微软雅黑, serif;">21.3%</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">的论文被</span><span style="font-size:16px;font-family:微软雅黑, serif;">ACL</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">主会录取，另有</span><span style="font-size:16px;font-family:微软雅黑, serif;">14.9%</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">的论文被</span><span style="font-size:16px;font-family:微软雅黑, serif;">Findings of ACL</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">录取。</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:center;margin-top:0.11in;margin-bottom:0.11in;"><img src="/_upload/article/images/16/c2/7c645da44504bd668c6993a6dc79/0694700d-abff-4a24-ae9b-fcc7d933eb1a.png" align="BOTTOM" width="554" height="223" border="0" /></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">人工智能（</span><span style="font-size:16px;font-family:微软雅黑, serif;">AI</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">）领域的顶级国际会议</span><span style="font-size:16px;font-family:微软雅黑, serif;">IJCAI 2021</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">（</span><span style="font-size:16px;font-family:微软雅黑, serif;">The 30th International Joint Conference on Artificial Intelligence</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">）将于</span><span style="font-size:16px;font-family:微软雅黑, serif;">2021</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">年</span><span style="font-size:16px;font-family:微软雅黑, serif;">8</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">月</span><span style="font-size:16px;font-family:微软雅黑, serif;">21-26</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">日在线举行（原定在加拿大蒙特利尔举行）。会议共计收到有效投稿</span><span style="font-size:16px;font-family:微软雅黑, serif;">4204</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">篇，最终共有</span><span style="font-size:16px;font-family:微软雅黑, serif;">587</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">篇论文被大会录用，总录用率为</span><span style="font-size:16px;font-family:微软雅黑, serif;">13.9%</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">。</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><br /></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">文章列表：</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>1.Title: An In-depth Study on Internal Structure of Chinese Words (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>词法句法</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>合作单位：华为</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (ACL)</strong></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>题目：</strong></span><span style="font-family:微软雅黑;">汉语词内部结构深层研究</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>作者：</strong></span><span style="font-family:微软雅黑;">龚晨，黄赛豪，周厚全，李正华，张民，王喆锋，怀宝兴，袁晶</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>摘要：</strong></span><span style="font-family:微软雅黑;">与英文字母不同，汉字有丰富而具体的含义。通常，一个词的意义在某种程度上可以由组成它的汉字派生出来。一些前人的句法分析工作提出对浅层词内部结构进行标注从而更好地利用字级别的信息。本文提出将汉语词的深层内部结构建模为包含</span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">11</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">个标签的依存树，用于区分词内部结构中不同的依存关系。首先，根据我们最新编写的标注规范，我们人工标注了一个来源于中文宾州树库的包含超过</span><span style="font-size:16px;font-family:微软雅黑, serif;">3</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">万个词的词内部结构 </span><span style="font-size:16px;font-family:微软雅黑, serif;">(WIST)</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">。为了保证标注质量，每个词都由两个标注者独立标注，由第三个标注者处理标注不一致情况。第二，我们对</span><span style="font-size:16px;font-family:微软雅黑, serif;">WIST</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">进行了详细而有趣的分析，揭示了对汉语构词的一些见解。第三，我们提出了词内部结构分析的新任务，并基于一个先进的句法分析器进行了基准实验。最后，我们提出了两种简单的编码词内部结构的方法，在句法分析任务中验证了汉语词内部结构的作用。</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>2.Title:&nbsp;More than Text: Multi-modal Chinese Word Segmentation (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>中文分词</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (ACL, Short Paper)</strong></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>题目：</strong></span><span style="font-family:微软雅黑;">不止于文本：多模态中文分词&nbsp;</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>作者：</strong></span><span style="font-family:微软雅黑;">张栋，胡政，李寿山，吴含前，朱巧明，周国栋</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>摘要：</strong></span><span style="font-family:微软雅黑;">毫无疑问，中文分词是自然语言处理中一项重要的基础性工作。以往的研究仅关注文本模态，但现实往往存在音频和视频内容（如新闻广播和面对面的对话），其中包括文本、声学和视觉模态。因此，我们尝试将多模态信息（本文中主要是转换后的文本和真实音频信息）结合起来进行分词。本文标注了一个包含文本和音频的全新中文分词数据集。此外，我们提出了一个基于</span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">Transformer</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">框架的时间依赖多模态交互模型，以整合多模态信息进行序列标注。在三种不同训练集上的实验结果表明，我们的方法将文本和音频按时间步融合在一起进行分词是有效的。</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>3.Title:&nbsp;XLPT-AMR: Cross-Lingual Pre-Training via Multi-Task Learning for Zero-Shot AMR Parsing and Text Generation (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>语义分析</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>合作单位：腾讯</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (ACL)</strong></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>题目：</strong></span><span style="font-family:微软雅黑;">基于多任务多语言预训练的零资源</span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">AMR</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">分析与文本生成</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>作者：</strong></span><span style="font-family:微软雅黑;">徐东钦，李军辉，朱慕华，张民，周国栋</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>摘要：</strong></span><span style="font-family:微软雅黑;">与英文</span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">AMR</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">的研究不同，由于缺少人工标注数据，其他语言</span><span style="font-size:16px;font-family:微软雅黑, serif;">AMR</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">的研究十分受限。为此，本文借助于预训练技术，提出了零资源跨语言</span><span style="font-size:16px;font-family:微软雅黑, serif;">AMR</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">分析与文本生成方案。具体地，将英语看作枢轴语言，提出了基于多任务学习的跨语言预训练方法。同时，基于得到的预训练模型，提出并比较了多种不同的微调方法。实验结果表明，借助该方法，本文方法极大地提高了现有的研究水平，在德语、西班牙语和意大利语的</span><span style="font-size:16px;font-family:微软雅黑, serif;">AMR</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">分析与文本生成的任务中提高了约</span><span style="font-size:16px;font-family:微软雅黑, serif;">10</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">个点的性能。</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>4.Title: Enhancing Entity Boundary Detection for Better Chinese Named Entity Recognition (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>命名实体识别</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (ACL,&nbsp;Short Paper)</strong></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>题目：</strong></span><span style="font-family:微软雅黑;">融入实体边界信息的中文命名实体识别</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>作者：</strong></span><span style="font-family:微软雅黑;">陈淳，孔芳</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>摘要：</strong></span><span style="font-family:微软雅黑;">命名实体识别旨在识别出自然语言文本中具有特定含义的实体，作为一个典型的序列标注问题，命名实体识别在长期发展中形成了</span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">Bi-LSTM+CRF</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">的深度学习框架。与英文相比，中文没有明确的词语边界和显式的实体边界变化，因此中文命名实体识别的形势更为严峻，有待进一步发展。近年来</span><span style="font-size:16px;font-family:微软雅黑, serif;">Transformer</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">在</span><span style="font-size:16px;font-family:微软雅黑, serif;">NLP</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">各领域都超越了</span><span style="font-size:16px;font-family:微软雅黑, serif;">RNN</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">类模型，但是在命名实体识别任务上效果不佳，我们使用轻量级的</span><span style="font-size:16px;font-family:微软雅黑, serif;">Star-Transformer</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">对序列进行结构化编码，作为</span><span style="font-size:16px;font-family:微软雅黑, serif;">baseline</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">。在此基础上，本文提出了一个融入实体边界信息的命名实体识别统一模型，从以下两个方面增强实体边界信息：</span><span style="font-size:16px;font-family:微软雅黑, serif;">1. </span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">使用图注意力网络</span><span style="font-size:16px;font-family:微软雅黑, serif;">(GAT)</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">对实体内部的词语依存关系进行编码，配合</span><span style="font-size:16px;font-family:微软雅黑, serif;">Star-Transformer</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">进一步捕获实体内部的语义特征；</span><span style="font-size:16px;font-family:微软雅黑, serif;">2. </span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">将实体边界识别看作是两个二分类任务（实体</span><span style="font-size:16px;font-family:微软雅黑, serif;">head</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">和</span><span style="font-size:16px;font-family:微软雅黑, serif;">tail</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">的识别）和命名实体识别同时训练，赋予命名实体识别任务明确的实体边界。实验表明，本文给出的模型在规范文本</span><span style="font-size:16px;font-family:微软雅黑, serif;">OntoNotes</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">和非规范文本</span><span style="font-size:16px;font-family:微软雅黑, serif;">Weibo</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">语料上都达到了</span><span style="font-size:16px;font-family:微软雅黑, serif;">SOTA</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">。</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>5.Title: Adversarial Learning for Discourse Rhetorical Structure Parsing (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">篇章分析</span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (ACL)</strong></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>题目：</strong></span><span style="font-family:微软雅黑;">面向篇章修辞结构解析的对抗学习算法</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>作者：</strong></span><span style="font-family:微软雅黑;">张龙印，孔芳，周国栋</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>摘要：</strong></span><span style="font-family:微软雅黑;">由于缺乏训练数据，文本级篇章修辞结构解析一直被认为是一个极具挑战性的研究课题。尽管最近提出的自顶向下的篇章修辞结构解析器可以很好地利用全局篇章上下文信息并取得了一定进展，但目前的解析性能仍然不够出色。据我们所知，以往所有的篇章修辞结构解析器均在每个解析步骤进行一次局部决策来实现自底向上的节点合成或者自顶向下的分割点排序，这在很大程度上忽略了从全局的角度进行篇章修辞结构解析。显然，仅通过这些局部决策来构建整个篇章修辞结构树是远远不够的。在这个工作中，我们期望通过评估整棵篇章树的优劣来实现对篇章修辞结构解析的全局优化。具体来说，基于最新的自顶向下的解析架构，我们提出了一种全新的方法将标准篇章树和自动篇章树分别转换为带双颜色通道的篇章树图。然后，我们在标准树图和自动树图之间训练一个对抗机器人来从全局的角度评估生成的篇章修辞结构树的优劣。我们在英文</span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">RST-DT</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">和中文</span><span style="font-size:16px;font-family:微软雅黑, serif;">CDTB</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">语料库上进行了实验，并使用原始</span><span style="font-size:16px;font-family:微软雅黑, serif;">Parseval</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">进行性能评估。实验结果表明，与最新的篇章修辞结构解析器相比，我们的解析器能够显著地提升解析性能。</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>6.Title: DuReader-Robust: A Chinese Dataset Towards Evaluating Robustness and Generalization of Machine Reading Comprehension in Real-World Application Scenarios (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>阅读理解</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>)&nbsp;(</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>合作单位：百度</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (ACL, Short Paper</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>）</strong></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>题目：</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;">DuReader-Robust</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">：评估真实应用场景下机器阅读理解鲁棒性与泛化性的中文数据集</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>作者：</strong></span><span style="font-family:微软雅黑;">唐竑轩，李弘宇，刘璟，洪宇，吴华，王海峰</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>摘要：</strong></span><span style="font-family:微软雅黑;">机器阅读理解（</span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">MRC</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">）是</span><span style="font-size:16px;font-family:微软雅黑, serif;">NLP</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">领域中一项至关重要且富有挑战的任务。本文提出了一个全新的中文机器阅读理解数据集</span><span style="font-size:16px;font-family:微软雅黑, serif;">DuReader-Robust</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">，旨在从过敏感、过稳定与泛化能力这三个方面评估现有</span><span style="font-size:16px;font-family:微软雅黑, serif;">MRC</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">模型在实际应用场景中的鲁棒性与泛化性。</span><span style="font-size:16px;font-family:微软雅黑, serif;">DuReader-Robust</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">中的全部样例均为真实用户提供的自然文本，本文将详细阐述该数据集的构建方式。此外，本文也基于</span><span style="font-size:16px;font-family:微软雅黑, serif;">DuReader-Robust</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">行了大量实验，希望通过这些实验启发未来的</span><span style="font-size:16px;font-family:微软雅黑, serif;">MRC</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">研究。最后，我们将公开</span><span style="font-size:16px;font-family:微软雅黑, serif;">DuReader-Robust</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">数据集以丰富中文</span><span style="font-size:16px;font-family:微软雅黑, serif;">MRC</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">的语料资源。</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>7.Title: Improving Text Generation with Dynamic Masking and Recovering (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>文本生成</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>合作单位：腾讯</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (IJCAI)</strong></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>题目：</strong></span><span style="font-family:微软雅黑;">利用动态覆盖和还原单词提高文本生成任务性能</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>作者：</strong></span><span style="font-family:微软雅黑;">刘志东，李军辉，朱慕华</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>摘要：</strong></span><span style="font-family:微软雅黑;">目前不同文本生成任务由于源端输入不同往往会采用不同的编码器</span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">-</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">解码器结构。因此大多数提高文本生成任务鲁棒性的方法都是受限于具体形式的输入，并不能很好适用于其他文本生成的任务。本文在基本的编码器解码器结构的基础上提出一种增强模型语义捕获能力的方法，该方法可以适用于不同的文本生成任务。具体做法为：首先，在训练过程中对目标端的文本序列按照一定比例进行随机覆盖，然后约束解码器除了要生成完整的目标序列还引入一个辅助任务用于还原被覆盖掉的单词。在机器翻译，</span><span style="font-size:16px;font-family:微软雅黑, serif;">AMR-to-Text</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">和图像描述三个文本生成任务的实验表明，该方法在不同输入形态的场景下也能显著提高文本生成任务的性能。</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>8.Title: Breaking the Corpus Bottleneck for Context-Aware Neural MachineTranslation with a Novel Joint Pre-training Approach (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>机器翻译</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>)&nbsp;(</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>合作单位：阿里</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>)&nbsp;(ACL)</strong></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>题目：</strong></span><span style="font-family:微软雅黑;">利用联合预训练任务打破上下文感知神经机器翻译语料缺乏瓶颈</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>作者：</strong></span><span style="font-family:微软雅黑;">陈林卿，李军辉，贡正仙，陈博兴，骆卫华，张民，周国栋</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>摘要：</strong></span><span style="font-family:微软雅黑;">上下文感知神经机器翻译（</span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">NMT</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">）由于缺乏大规模文档级平行数据仍然具有挑战性。为了突破语料库瓶颈，本文利用大规模句子级平行数据集和源端单语文档改进上下文感知的</span><span style="font-size:16px;font-family:微软雅黑, serif;">NMT</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">。为此目的，我们提出两项预训练任务。一个在句子级平行数据集中学习将一个句子从源语言翻译成目标语言，而另一个在单语文档上学习将一个文档从加噪版本翻译成原始版本。这两个预训练任务是通过同一模型共同学习的，然后利用规模有限的平行文档语料从句子级和文档级的角度对模型进行微调。在四个翻译任务上的实验结果表明，我们的方法显著提高了翻译性能。我们的方法的一个很好的特性是，经过微调的模型可以用于翻译句子和文档。</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>9.Title: Improving Context-Aware Neural Machine Translation with Source-side Monolingual Documents (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>机器翻译</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>合作单位：阿里</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (IJCAI)</strong></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>题目：</strong></span><span style="font-family:微软雅黑;">通过源端单语篇章语料提高上下文感知神经机器翻译性能</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>作者：</strong></span><span style="font-family:微软雅黑;">陈林卿，李军辉，贡正仙，段湘煜，陈博兴，骆卫华，张民，周国栋</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>摘要：</strong></span><span style="font-family:微软雅黑;">为了充分利用源端单语文档实现上下文感知的</span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">NMT</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">，我们首次提出一种新的自监督预训练任务，该任务包含两个训练目标：</span><span style="font-size:16px;font-family:微软雅黑, serif;">(1)</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">从破损的句子中重建原句；</span><span style="font-size:16px;font-family:微软雅黑, serif;">(2)</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">从左右相邻的句子中生成一个间隙句。然后我们设计了一个通用的预训练全局上下文（</span><span style="font-size:16px;font-family:微软雅黑, serif;">PGC</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">）模型，该模型由一个全局上下文编码器、一个句子编码器和一个解码器组成，其结构与典型的上下文感知</span><span style="font-size:16px;font-family:微软雅黑, serif;">NMT</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">模型相似。我们通过将预先训练好的</span><span style="font-size:16px;font-family:微软雅黑, serif;">PGC</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">模型适应于各种下游上下文感知的</span><span style="font-size:16px;font-family:微软雅黑, serif;">NMT</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">模型来评估其有效性和通用性。对四种不同翻译任务的实验表明，我们的</span><span style="font-size:16px;font-family:微软雅黑, serif;">PGC</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">方法显著提高了上下文感知的</span><span style="font-size:16px;font-family:微软雅黑, serif;">NMT</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">的翻译性能。例如，基于最先进的</span><span style="font-size:16px;font-family:微软雅黑, serif;">SAN</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">模型，我们在四个翻译任务上平均提高了</span><span style="font-size:16px;font-family:微软雅黑, serif;">1.85 BLEU</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">值和</span><span style="font-size:16px;font-family:微软雅黑, serif;">1.59 Meteor</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">值。</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>10.Title: Combining Static Word Embeddings and Contextual Representations for Bilingual Lexicon Induction (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>机器翻译</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>合作单位：阿里</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>)&nbsp;(Findings of ACL)</strong></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>题目：</strong></span><span style="font-family:微软雅黑;">结合静态词向量与上下文表示的双语词典归纳</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>作者：</strong></span><span style="font-family:微软雅黑;">张金鹏，季佰军，肖妮妮，段湘煜，张民，施杨斌，骆卫华</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>摘要：</strong></span><span style="font-family:微软雅黑;">双语词典归纳（</span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">BLI</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">）旨在寻找源语言单词在目标语言中的翻译，其最典型的方法是通过学习线性变换来对齐单词表示空间。目前，两类典型的词向量表示方法，即静态词向量和上下文向量，已经被用于探索双语词典归纳，但是以前的研究并没有将这两类方法有效的结合起来。因此，本文提出了一种简单且有效的机制来结合静态词向量与上下文向量，这一机制可以在进行双语词典归纳时充分发挥两者各自的优点。我们分别在无监督与有监督的基准设置上进行实验，测试了这一结合机制在多种语言对上的双语词典归纳效果，实验结果表明我们的方法可以在各种语言对上获得稳定的提升，在有监督设置中，平均可以提升</span><span style="font-size:16px;font-family:微软雅黑, serif;">3.2</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">个百分点，在无监督设置中，平均可以提升</span><span style="font-size:16px;font-family:微软雅黑, serif;">3.1</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">个百分点，显著超越了现有系统。</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>11.Title: LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>预训练</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>合作单位：哈工大、微软</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (ACL)</strong></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>题目：</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;">LayoutLMv2: </span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">面向富视觉信息文档理解的多模态预训练</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>作者：</strong></span><span style="font-family:微软雅黑;">徐阳，徐毅恒，吕腾超，崔磊，韦福如，</span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">Guoxin Wang</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">，</span><span style="font-size:16px;font-family:微软雅黑, serif;">Yijuan Lu</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">，</span><span style="font-size:16px;font-family:微软雅黑, serif;">Dinei Florencio</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">，</span><span style="font-size:16px;font-family:微软雅黑, serif;">Cha Zhang</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">，车万翔，张民，周礼栋</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>摘要：</strong></span><span style="font-family:微软雅黑;">现实生活中种类繁多的文档（扫描或电子版的票据、报告、文件等）往往会包含布局、图像等丰富的视觉信息，单纯从文本入手难以充分挖掘其内容。本文提出的 </span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">LayoutLMv2 </span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">使用带有空间感知自注意力机制的多模态 </span><span style="font-size:16px;font-family:微软雅黑, serif;">Transformer </span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">编码器网络，统一建模文本、图像、布局三种模态的信息，能够在大规模无标注文档数据上预训练通用文档理解模型。在预训练任务方面，除了现有的遮罩式视觉语言模型（</span><span style="font-size:16px;font-family:微软雅黑, serif;">Masked Visual-Language Modeling</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">）外，</span><span style="font-size:16px;font-family:微软雅黑, serif;">LayoutLMv2 </span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">还引入了新的文本—图像对齐（</span><span style="font-size:16px;font-family:微软雅黑, serif;">Text-Image Alignment</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">）以及文本—图像匹配（</span><span style="font-size:16px;font-family:微软雅黑, serif;">Text-Image Matching</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">）任务，帮助模型从位置和语义层面对齐多模态信息。实验结果表明，经过预训练—微调的 </span><span style="font-size:16px;font-family:微软雅黑, serif;">LayoutLMv2 </span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">模型在对应不同类型任务的六个文档理解数据集上显著优于基线方法，达到世界领先水平。</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>12.Title: Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>篇章理解、预训练</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>合作单位：厦门大学、阿里</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (ACL)</strong></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>题目：</strong></span><span style="font-family:微软雅黑;">解决自然语言生成预训练</span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">-</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">微调范式中的字词粒度差异问题</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>作者：</strong></span><span style="font-family:微软雅黑;">刘鑫，杨宝嵩，刘大一恒，张海波，骆卫华，张民，张海英，苏劲松</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>摘要：</strong></span><span style="font-family:微软雅黑;">预训练</span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">-</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">微调范式的一个局限性在于其固定的词表带来的不灵活。这一局限削弱了预训练模型应用至自然语言生成任务时的效果，尤其是上下游任务的子词分布存在着明显差异时。为了解决这一问题，我们在传统预训练</span><span style="font-size:16px;font-family:微软雅黑, serif;">-</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">微调范式基础上引入了一个词嵌入转移步骤。具体而言，我们设计了一种即插即用的词嵌入生成器用于生成任意输入词语的词嵌入，这一过程生成器将参照与其形态相似的预训练词表词嵌入。因此，下游任务中与上游任务不匹配词语的词嵌入可以被有效地初始化。我们在基于多个生成式任务的预训练</span><span style="font-size:16px;font-family:微软雅黑, serif;">-</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">微调范式下进行了实验。实验结果和分析表明我们提出的策略能够保证上下游任务词表的自由迁移，进而在下游生成任务上提升了模型效果。</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>13.Title:&nbsp;A Structure Self-Aware Model for Discourse Parsing on Multi-Party Dialogues (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>篇章分析</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>合作单位：厦门大学、腾讯、阿里</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>)&nbsp;&nbsp;(IJCAI)</strong></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>题目：</strong></span><span style="font-family:微软雅黑;">针对多人对话篇章解析任务的结构自感知模型</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>作者：</strong></span><span style="font-family:微软雅黑;">王安特，宋霖峰，蒋辉，赖少鹏，姚俊峰，张民，苏劲松</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>摘要：</strong></span><span style="font-family:微软雅黑;">对话篇章结构描述了对话的组织方式，因此对对话的理解与生成有所帮助。本文关注多人对话场景下的篇章结构预测。之前的工作采用了增量式的生成方法，利用历史预测结构信息辅助待预测关系生成。尽管这种方式考虑到了结构关系间的关联信息，但同时错误传播问题也非常严重并对性能造成影响。为了缓解错误传播问题，我们提出了一种结构自感知模型。该模型采用了关注结构关系的图神经网络，更新句对间的结构信息，学习更加直接的结构表示。并且，我们采用了额外的训练信号来辅助表示学习。我们在两个对话篇章解析数据集上验证了模型的有效性。</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>14.Title: BASS: Boosting Abstractive Summarization with Unified Semantic Graph (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>文本摘要</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>合作单位：北大、百度</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (ACL)</strong></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>题目：</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;">BASS:</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">基于统一语义图的生成式文本摘要</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>作者：</strong></span><span style="font-family:微软雅黑;">吴文浩，李伟，肖欣延，刘家辰，曹自强，李素建，吴华，王海峰</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>摘要：</strong></span><span style="font-family:微软雅黑;">对于</span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">Seq2Seq</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">结构而言，长文档或多文档的生成式摘要仍然十分具有挑战性，因为</span><span style="font-size:16px;font-family:微软雅黑, serif;">Seq2Seq</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">不擅长学习文本中的长距离关系。在本文中，我们介绍了</span><span style="font-size:16px;font-family:微软雅黑, serif;">BASS</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">模型，它是一种基于统一语义图的生成式摘要框架。该框架利用统一语义图聚合分布在很长上下文中的共指短语，并挖掘了短语之间的丰富关系。此外，本文提出了一种基于图的编码器</span><span style="font-size:16px;font-family:微软雅黑, serif;">-</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">解码器模型，以利用图的结构来改进文档表示和摘要生成过程。具体而言，本文在编码过程中设计了一些图增强方法，在解码过程中设计了图传播注意机制以帮助摘要生成中的内容选择与语言组织。实证结果表明，本文所提出的方法为长文档和多文档摘要任务带来了明显的改进。</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>15.Title: Code Summarization with Structure-induced Transformer (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>摘要生成</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>合作单位：上海交大</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (Findings of ACL)</strong></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>题目：</strong></span><span style="font-family:微软雅黑;">基于结构诱导</span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">Transformer</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">的代码摘要生成</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>作者：</strong></span><span style="font-family:微软雅黑;">伍鸿秋，&nbsp;赵海， 张民</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>摘要：</strong></span><span style="font-family:微软雅黑;">在如今的语言理解领域，代码摘要（</span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">Code summarization</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">）是一个极具潜力的研究方向。它旨在为程序语言生成合理的注释，以此极力地助于程序开发工作。程序语言是十分结构化的，因此前人们尝试通过结构遍历（</span><span style="font-size:16px;font-family:微软雅黑, serif;">structure-based traversal</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">）或非序列化模型，如树形</span><span style="font-size:16px;font-family:微软雅黑, serif;">LSTM</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">、图神经网络，去学习程序的语义。然而令人惊讶的是，在如</span><span style="font-size:16px;font-family:微软雅黑, serif;">Transformer</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">这样的相比</span><span style="font-size:16px;font-family:微软雅黑, serif;">LSTM</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">更为先进的编码器中融入结构遍历并不能得到性能提升，这使得图神经网络成为了建模该类结构信息的唯一方法。为了减少这种不便，我们提出了结构诱导</span><span style="font-size:16px;font-family:微软雅黑, serif;">Transformer</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">（</span><span style="font-size:16px;font-family:微软雅黑, serif;">structure-induced Transformer</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">），它通过全新的结构诱导自注意力机制，对代码序列的多视角结构信息编码。通过大量的实验，我们证明了结构诱导</span><span style="font-size:16px;font-family:微软雅黑, serif;">Transformer</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">能够在基准数据集上达到新的</span><span style="font-size:16px;font-family:微软雅黑, serif;">SOTA</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">。</span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><a name="_GoBack"></a><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>16.Title: Enhancing the Open-Domain Dialogue Evaluation in Latent Space (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>对话评测</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>)&nbsp; (</strong></span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;"><strong>合作单位：北大、腾讯、人大</strong></span><span style="font-size:16px;font-family:微软雅黑, serif;"><strong>) (Findings of ACL)</strong></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>题目：</strong></span><span style="font-family:微软雅黑;">隐空间增强的开放域对话评测</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>作者：</strong></span><span style="font-family:微软雅黑;">产张明，刘乐茂，李俊涛，张海松，赵东岩，史树明，严睿</span></span></span></p><p style="text-indent:0.33in;line-height:200%;widows:2;orphans:2;text-align:justify;margin-top:0.11in;margin-bottom:0.11in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN"><span style="font-family:微软雅黑;"><strong>摘要：</strong></span><span style="font-family:微软雅黑;">开放域对话中“</span></span></span><span style="font-size:16px;font-family:微软雅黑, serif;">one-to-many”</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">特性导致了其自动评估方法的设计成为一个巨大的挑战。最近的研究试图通过直接考虑生成的回复与对话上下文的匹配度来解决该问题，并利用判别模型从多个正样本中学习。尽管这类方法取得了令人兴奋的进展，但它们无法应用于没有多个合理回复的训练数据——而这正是真实世界数据集的一般情况。为此，我们提出通过隐空间建模增强的对话评估指标——</span><span style="font-size:16px;font-family:微软雅黑, serif;">EMS</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">。具体来说，我们利用自监督学习来获得一个平滑的隐空间，该空间既可提取对话的上下文信息，也可以对该上下文可能的合理回复进行建模。然后我们利用隐空间中捕捉的信息对对话评测过程进行增强。在两个真实世界对话数据集上的实验结果证明了我们方法的优越性，其中与人类判断相关的</span><span style="font-size:16px;font-family:微软雅黑, serif;">Pearson</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">和</span><span style="font-size:16px;font-family:微软雅黑, serif;">Spearman</span><span style="font-family:宋体;font-size:16px;font-size:16px;font-family:微软雅黑;">相关性分数均胜过所有基线模型。</span></p><p style="widows:2;orphans:2;text-align:left;margin-top:0.02in;margin-bottom:0.02in;"><br /><br /></p><p style="widows:2;orphans:2;text-align:right;margin-top:0.02in;margin-bottom:0.02in;"><span style="font-family:宋体;font-size:16px;"><span lang="zh-CN">编辑：杨浩苹，龚晨，李正华</span></span></p><p><br /></p></div></div>
				  </div>
				
			</div>
		</div>
  </div>
</div>
<!--//主体结束-->
<!--底部开始-->
<div class="wrapper footer" id="footer">
	<div class="inner">
		<div class="mod clearfix">
			<div class="xlink" frag="窗口91">		
				
					
					<ul class="">
						
						<li class="links i1"><a href="http://csteaching.suda.edu.cn" target="_blank">教学辅助平台</a> </li>
						
						<li class="links i2"><a href="http://192.168.125.2:9088/tm/Default.aspx" target="_blank">学院经费管理系统（校内）</a> </li>
						
						<li class="links i3"><a href="http://42.244.43.76:8080/" target="_self">科研成果申报平台（校内）</a> </li>
						
						<li class="links i4"><a href="http://ciptlab.suda.edu.cn/" target="_blank">省级重点实验室</a> </li>
						
						<li class="links i5"><a href="http://sit.suda.edu.cn/" target="_blank">计算机教学网（校内）</a> </li>
						
						<li class="links i6"><a href="http://nlp.suda.edu.cn/" target="_blank">自然语言处理实验室</a> </li>
						
					</ul>
					
				
			</div>
			<div class="foot-left"> 
				<p class="copyright"><span>Copyright 苏州大学计算机科学与技术学院 2018 All Rights Reserved </span><span>苏州市十梓街1号</span></p>
				<p class="copyright"><span>苏ICP备-10229414  苏公网安备 32050802010530号</span></p>
			</div>
		</div>
	</div>
</div>
<!--//底部结束-->
<script type="text/javascript" src="/_upload/tpl/01/db/475/template475/js/comcus.js"></script>
<script type="text/javascript" src="/_upload/tpl/01/db/475/template475/js/list.js"></script>
<script type="text/javascript" src="/_upload/tpl/01/db/475/template475/js/app.js"></script>
<script type="text/javascript">
$(function(){
	new SDAPP({
		"menu":{
			type:"slide"
		},
		"view":{
			target:".read img",
			minSize:40
		}
	});
});
</script>
</body>
</html>
 <img src="/_visitcount?siteId=289&type=3&articleId=444006" style="display:none" width="0" height="0"/>